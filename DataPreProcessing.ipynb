{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5393c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import all required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "import pyedflib\n",
    "import mne\n",
    "from mne import filtering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and dataset exploration\n",
    "class CHBMITConfig:\n",
    "    # Dataset paths (adjust based on your actual path)\n",
    "    DATA_PATH = \"chb-mit-scalp-eeg-database-1.0.0\"  # Update this path\n",
    "    \n",
    "    # Signal parameters\n",
    "    TARGET_SAMPLING_RATE = 256  # Hz\n",
    "    WINDOW_SIZE_PREDICTION = 10  # seconds\n",
    "    WINDOW_SIZE_DETECTION = 4   # seconds\n",
    "    WINDOW_STRIDE = 1           # seconds\n",
    "    \n",
    "    # Preprocessing parameters\n",
    "    HIGH_PASS_FREQ = 0.5  # Hz\n",
    "    LOW_PASS_FREQ = 70    # Hz\n",
    "    NOTCH_FREQ = 60       # Hz (for US power line)\n",
    "    \n",
    "    # Pre-ictal period definition\n",
    "    PRE_ICTAL_PERIOD = 300  # 5 minutes before seizure\n",
    "    \n",
    "    # Train/val/test split\n",
    "    TRAIN_RATIO = 0.7\n",
    "    VAL_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "\n",
    "config = CHBMITConfig()\n",
    "\n",
    "# Explore dataset structure\n",
    "def explore_dataset_structure(data_path):\n",
    "    \"\"\"Explore the CHB-MIT dataset structure\"\"\"\n",
    "    patients = []\n",
    "    \n",
    "    for item in os.listdir(data_path):\n",
    "        item_path = os.path.join(data_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            patients.append(item)\n",
    "    \n",
    "    print(f\"Found {len(patients)} patients/subjects\")\n",
    "    print(\"Patients:\", sorted(patients))\n",
    "    \n",
    "    # Explore one patient to understand file structure\n",
    "    if patients:\n",
    "        sample_patient = patients[0]\n",
    "        sample_path = os.path.join(data_path, sample_patient)\n",
    "        print(f\"\\nExploring {sample_patient}:\")\n",
    "        \n",
    "        for file in os.listdir(sample_path):\n",
    "            if file.endswith('.edf') or file.endswith('.seizures') or file.endswith('.txt'):\n",
    "                print(f\"  {file}\")\n",
    "    \n",
    "    return patients\n",
    "\n",
    "patients = explore_dataset_structure(config.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Utility functions for data parsing\n",
    "def parse_summary_file(patient_path):\n",
    "    \"\"\"Parse the summary file for seizure information\"\"\"\n",
    "    summary_files = [f for f in os.listdir(patient_path) if 'summary' in f.lower() and f.endswith('.txt')]\n",
    "    \n",
    "    if not summary_files:\n",
    "        print(f\"No summary file found for {patient_path}\")\n",
    "        return None\n",
    "    \n",
    "    summary_file = os.path.join(patient_path, summary_files[0])\n",
    "    seizure_info = {}\n",
    "    \n",
    "    try:\n",
    "        with open(summary_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        current_file = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('File Name:'):\n",
    "                current_file = line.split(': ')[1]\n",
    "                seizure_info[current_file] = []\n",
    "            elif line.startswith('Seizure Start Time:'):\n",
    "                parts = line.split()\n",
    "                start_time = int(parts[3])\n",
    "                end_time = int(parts[5]) if len(parts) > 5 else start_time + 1\n",
    "                seizure_info[current_file].append((start_time, end_time))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing summary file {summary_file}: {e}\")\n",
    "    \n",
    "    return seizure_info\n",
    "\n",
    "def parse_seizure_file(seizure_file_path):\n",
    "    \"\"\"Parse individual seizure files\"\"\"\n",
    "    seizures = []\n",
    "    try:\n",
    "        with open(seizure_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    start_time = int(parts[0])\n",
    "                    end_time = int(parts[1])\n",
    "                    seizures.append((start_time, end_time))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return seizures\n",
    "\n",
    "def get_all_seizure_info(patient_path):\n",
    "    \"\"\"Get all seizure information for a patient\"\"\"\n",
    "    seizure_info = parse_summary_file(patient_path)\n",
    "    \n",
    "    # Also check individual seizure files\n",
    "    seizure_files = [f for f in os.listdir(patient_path) if f.endswith('.seizures')]\n",
    "    \n",
    "    for seizure_file in seizure_files:\n",
    "        file_base = seizure_file.replace('.seizures', '')\n",
    "        seizures = parse_seizure_file(os.path.join(patient_path, seizure_file))\n",
    "        if seizures:\n",
    "            if file_base not in seizure_info:\n",
    "                seizure_info[file_base] = []\n",
    "            seizure_info[file_base].extend(seizures)\n",
    "    \n",
    "    return seizure_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: EEG Data Loader and Preprocessor\n",
    "class CHBMITDataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.standard_channels = None\n",
    "        \n",
    "    def load_edf_file(self, file_path):\n",
    "        \"\"\"Load EDF file and return data and info\"\"\"\n",
    "        try:\n",
    "            # Try using pyedflib first\n",
    "            f = pyedflib.EdfReader(file_path)\n",
    "            n_channels = f.signals_in_file\n",
    "            channel_names = f.getSignalLabels()\n",
    "            fs = f.getSampleFrequency(0)  # Assuming same sampling rate for all channels\n",
    "            \n",
    "            # Read data\n",
    "            data = np.zeros((n_channels, f.getNSamples()[0]))\n",
    "            for i in range(n_channels):\n",
    "                data[i, :] = f.readSignal(i)\n",
    "            \n",
    "            f.close()\n",
    "            return data, channel_names, fs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"pyedflib failed for {file_path}: {e}. Trying MNE...\")\n",
    "            try:\n",
    "                raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "                data = raw.get_data()\n",
    "                channel_names = raw.ch_names\n",
    "                fs = raw.info['sfreq']\n",
    "                return data, channel_names, fs\n",
    "            except Exception as e2:\n",
    "                print(f\"MNE also failed for {file_path}: {e2}\")\n",
    "                return None, None, None\n",
    "    \n",
    "    def preprocess_signal(self, data, original_fs, channel_names):\n",
    "        \"\"\"Preprocess EEG signal: resample, filter, standardize channels\"\"\"\n",
    "        n_channels, n_samples = data.shape\n",
    "        \n",
    "        # Step 1: Resample to target frequency if needed\n",
    "        if original_fs != self.config.TARGET_SAMPLING_RATE:\n",
    "            data_resampled = np.zeros((n_channels, \n",
    "                                     int(n_samples * self.config.TARGET_SAMPLING_RATE / original_fs)))\n",
    "            for i in range(n_channels):\n",
    "                data_resampled[i] = signal.resample(data[i], \n",
    "                                                  int(n_samples * self.config.TARGET_SAMPLING_RATE / original_fs))\n",
    "            data = data_resampled\n",
    "            fs = self.config.TARGET_SAMPLING_RATE\n",
    "        else:\n",
    "            fs = original_fs\n",
    "        \n",
    "        # Step 2: Standardize channel names and order\n",
    "        data = self.standardize_channels(data, channel_names, fs)\n",
    "        if data is None:\n",
    "            return None, None\n",
    "        \n",
    "        # Step 3: Apply filters\n",
    "        data_filtered = self.apply_filters(data, fs)\n",
    "        \n",
    "        return data_filtered, fs\n",
    "    \n",
    "    def standardize_channels(self, data, channel_names, fs):\n",
    "        \"\"\"Standardize channel names and select common channels\"\"\"\n",
    "        if self.standard_channels is None:\n",
    "            # Define standard channel set based on common EEG montage\n",
    "            self.standard_channels = [\n",
    "                'FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1',\n",
    "                'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2',\n",
    "                'FZ-CZ', 'CZ-PZ'\n",
    "            ]\n",
    "        \n",
    "        # Map actual channel names to standard names\n",
    "        channel_mapping = {}\n",
    "        for i, ch in enumerate(channel_names):\n",
    "            ch_upper = ch.upper()\n",
    "            # Remove spaces and special characters\n",
    "            ch_clean = ch_upper.replace(' ', '').replace('-', '').replace('_', '')\n",
    "            \n",
    "            for std_ch in self.standard_channels:\n",
    "                std_clean = std_ch.replace('-', '')\n",
    "                if std_clean in ch_clean or ch_clean in std_clean:\n",
    "                    channel_mapping[std_ch] = i\n",
    "                    break\n",
    "        \n",
    "        # Create standardized data array\n",
    "        standardized_data = np.zeros((len(self.standard_channels), data.shape[1]))\n",
    "        channels_found = []\n",
    "        \n",
    "        for j, std_ch in enumerate(self.standard_channels):\n",
    "            if std_ch in channel_mapping:\n",
    "                standardized_data[j] = data[channel_mapping[std_ch]]\n",
    "                channels_found.append(std_ch)\n",
    "            else:\n",
    "                # If channel not found, use zeros (will be handled later)\n",
    "                standardized_data[j] = np.zeros(data.shape[1])\n",
    "        \n",
    "        print(f\"Found {len(channels_found)}/{len(self.standard_channels)} standard channels\")\n",
    "        return standardized_data\n",
    "    \n",
    "    def apply_filters(self, data, fs):\n",
    "        \"\"\"Apply bandpass and notch filters\"\"\"\n",
    "        # Bandpass filter\n",
    "        nyquist = fs / 2\n",
    "        low = self.config.HIGH_PASS_FREQ / nyquist\n",
    "        high = self.config.LOW_PASS_FREQ / nyquist\n",
    "        \n",
    "        if low > 0 and high < 1:\n",
    "            b, a = signal.butter(4, [low, high], btype='band')\n",
    "            data_filtered = signal.filtfilt(b, a, data, axis=1)\n",
    "        else:\n",
    "            data_filtered = data\n",
    "        \n",
    "        # Notch filter for power line interference\n",
    "        if self.config.NOTCH_FREQ > 0:\n",
    "            notch_freq = self.config.NOTCH_FREQ\n",
    "            quality = 30  # Quality factor\n",
    "            b, a = signal.iirnotch(notch_freq, quality, fs)\n",
    "            data_filtered = signal.filtfilt(b, a, data_filtered, axis=1)\n",
    "        \n",
    "        return data_filtered\n",
    "    \n",
    "    def normalize_signal(self, data):\n",
    "        \"\"\"Normalize signal using robust scaling\"\"\"\n",
    "        # Use median and MAD for robust normalization\n",
    "        median = np.median(data, axis=1, keepdims=True)\n",
    "        mad = np.median(np.abs(data - median), axis=1, keepdims=True)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        mad[mad == 0] = 1.0\n",
    "        \n",
    "        normalized = (data - median) / mad\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Label Generation and Dataset Creation\n",
    "class SeizureDatasetGenerator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data_loader = CHBMITDataLoader(config)\n",
    "    \n",
    "    def generate_labels_for_file(self, data_length, fs, seizure_intervals, file_duration):\n",
    "        \"\"\"Generate detection and prediction labels for a file\"\"\"\n",
    "        n_samples = data_length\n",
    "        total_seconds = n_samples / fs\n",
    "        \n",
    "        # Initialize labels\n",
    "        detection_labels = np.zeros(n_samples)\n",
    "        prediction_labels = np.zeros(n_samples)\n",
    "        \n",
    "        # Mark seizure periods for detection\n",
    "        for start_sec, end_sec in seizure_intervals:\n",
    "            start_sample = int(start_sec * fs)\n",
    "            end_sample = min(int(end_sec * fs), n_samples)\n",
    "            detection_labels[start_sample:end_sample] = 1\n",
    "            \n",
    "            # Mark pre-ictal period for prediction\n",
    "            pre_ictal_start = max(0, start_sec - self.config.PRE_ICTAL_PERIOD)\n",
    "            pre_ictal_end = start_sec\n",
    "            pre_ictal_start_sample = int(pre_ictal_start * fs)\n",
    "            pre_ictal_end_sample = int(pre_ictal_end * fs)\n",
    "            prediction_labels[pre_ictal_start_sample:pre_ictal_end_sample] = 1\n",
    "        \n",
    "        return detection_labels, prediction_labels\n",
    "    \n",
    "    def create_windowed_dataset(self, data, detection_labels, prediction_labels, fs, window_type=\"prediction\"):\n",
    "        \"\"\"Create windowed dataset from continuous data\"\"\"\n",
    "        if window_type == \"prediction\":\n",
    "            window_size = self.config.WINDOW_SIZE_PREDICTION\n",
    "        else:  # detection\n",
    "            window_size = self.config.WINDOW_SIZE_DETECTION\n",
    "            \n",
    "        window_samples = int(window_size * fs)\n",
    "        stride_samples = int(self.config.WINDOW_STRIDE * fs)\n",
    "        \n",
    "        windows = []\n",
    "        det_labels = []\n",
    "        pred_labels = []\n",
    "        metadata = []  # Store start time, file info, etc.\n",
    "        \n",
    "        n_samples = data.shape[1]\n",
    "        \n",
    "        for start_idx in range(0, n_samples - window_samples + 1, stride_samples):\n",
    "            end_idx = start_idx + window_samples\n",
    "            \n",
    "            window_data = data[:, start_idx:end_idx]\n",
    "            \n",
    "            # Get majority label for the window\n",
    "            det_label = np.mean(detection_labels[start_idx:end_idx]) > 0.5\n",
    "            pred_label = np.mean(prediction_labels[start_idx:end_idx]) > 0.5\n",
    "            \n",
    "            # Only include windows with sufficient signal quality (optional)\n",
    "            if self.is_valid_window(window_data):\n",
    "                windows.append(window_data)\n",
    "                det_labels.append(det_label)\n",
    "                pred_labels.append(pred_label)\n",
    "                metadata.append({\n",
    "                    'start_sample': start_idx,\n",
    "                    'end_sample': end_idx,\n",
    "                    'start_time': start_idx / fs,\n",
    "                    'end_time': end_idx / fs\n",
    "                })\n",
    "        \n",
    "        return np.array(windows), np.array(det_labels), np.array(pred_labels), metadata\n",
    "    \n",
    "    def is_valid_window(self, window_data):\n",
    "        \"\"\"Check if window has valid EEG data\"\"\"\n",
    "        # Check for flat lines (equipment failure)\n",
    "        if np.max(window_data) - np.min(window_data) < 1e-6:\n",
    "            return False\n",
    "        \n",
    "        # Check for excessive amplitude (artifacts)\n",
    "        if np.max(np.abs(window_data)) > 1000:  # Adjust threshold as needed\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def process_patient(self, patient_path):\n",
    "        \"\"\"Process all files for a single patient\"\"\"\n",
    "        all_windows = []\n",
    "        all_det_labels = []\n",
    "        all_pred_labels = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        seizure_info = get_all_seizure_info(patient_path)\n",
    "        if not seizure_info:\n",
    "            print(f\"No seizure information found for {patient_path}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        edf_files = [f for f in os.listdir(patient_path) if f.endswith('.edf')]\n",
    "        \n",
    "        for edf_file in sorted(edf_files):\n",
    "            file_path = os.path.join(patient_path, edf_file)\n",
    "            print(f\"Processing {edf_file}...\")\n",
    "            \n",
    "            # Load and preprocess data\n",
    "            data, channel_names, original_fs = self.data_loader.load_edf_file(file_path)\n",
    "            if data is None:\n",
    "                continue\n",
    "                \n",
    "            processed_data, fs = self.data_loader.preprocess_signal(data, original_fs, channel_names)\n",
    "            if processed_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Normalize data\n",
    "            normalized_data = self.data_loader.normalize_signal(processed_data)\n",
    "            \n",
    "            # Get seizure intervals for this file\n",
    "            file_key = edf_file.replace('.edf', '')\n",
    "            seizure_intervals = seizure_info.get(file_key, [])\n",
    "            \n",
    "            # Generate labels\n",
    "            detection_labels, prediction_labels = self.generate_labels_for_file(\n",
    "                normalized_data.shape[1], fs, seizure_intervals, \n",
    "                normalized_data.shape[1] / fs\n",
    "            )\n",
    "            \n",
    "            # Create windows for prediction task\n",
    "            windows, det_labels, pred_labels, metadata = self.create_windowed_dataset(\n",
    "                normalized_data, detection_labels, prediction_labels, fs, \"prediction\"\n",
    "            )\n",
    "            \n",
    "            # Add file info to metadata\n",
    "            for meta in metadata:\n",
    "                meta['file'] = edf_file\n",
    "                meta['patient'] = os.path.basename(patient_path)\n",
    "            \n",
    "            all_windows.append(windows)\n",
    "            all_det_labels.append(det_labels)\n",
    "            all_pred_labels.append(pred_labels)\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            print(f\"  Created {len(windows)} windows, \"\n",
    "                  f\"Seizure: {np.sum(det_labels)}, Pre-ictal: {np.sum(pred_labels)}\")\n",
    "        \n",
    "        if all_windows:\n",
    "            all_windows = np.vstack(all_windows)\n",
    "            all_det_labels = np.concatenate(all_det_labels)\n",
    "            all_pred_labels = np.concatenate(all_pred_labels)\n",
    "            \n",
    "            return all_windows, all_det_labels, all_pred_labels, all_metadata\n",
    "        else:\n",
    "            return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0968f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Complete Dataset Preprocessing Pipeline\n",
    "class CHBMITPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.dataset_generator = SeizureDatasetGenerator(config)\n",
    "    \n",
    "    def preprocess_complete_dataset(self):\n",
    "        \"\"\"Preprocess entire CHB-MIT dataset\"\"\"\n",
    "        patients = []\n",
    "        for item in os.listdir(self.config.DATA_PATH):\n",
    "            item_path = os.path.join(self.config.DATA_PATH, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                patients.append(item_path)\n",
    "        \n",
    "        all_patient_data = {}\n",
    "        \n",
    "        for patient_path in sorted(patients):\n",
    "            patient_id = os.path.basename(patient_path)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing patient: {patient_id}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            windows, det_labels, pred_labels, metadata = self.dataset_generator.process_patient(patient_path)\n",
    "            \n",
    "            if windows is not None:\n",
    "                all_patient_data[patient_id] = {\n",
    "                    'windows': windows,\n",
    "                    'detection_labels': det_labels,\n",
    "                    'prediction_labels': pred_labels,\n",
    "                    'metadata': metadata\n",
    "                }\n",
    "                \n",
    "                print(f\"Patient {patient_id}: {windows.shape[0]} windows, \"\n",
    "                      f\"{np.sum(det_labels)} seizure windows, \"\n",
    "                      f\"{np.sum(pred_labels)} pre-ictal windows\")\n",
    "                \n",
    "                # Save individual patient data\n",
    "                self.save_patient_data(patient_id, windows, det_labels, pred_labels, metadata)\n",
    "            else:\n",
    "                print(f\"Skipping patient {patient_id} - no valid data\")\n",
    "        \n",
    "        return all_patient_data\n",
    "    \n",
    "    def save_patient_data(self, patient_id, windows, det_labels, pred_labels, metadata):\n",
    "        \"\"\"Save preprocessed data for a patient\"\"\"\n",
    "        save_dir = f\"./preprocessed_data\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        save_path = f\"{save_dir}/{patient_id}_preprocessed.npz\"\n",
    "        \n",
    "        np.savez_compressed(\n",
    "            save_path,\n",
    "            windows=windows.astype(np.float32),\n",
    "            detection_labels=det_labels.astype(np.int8),\n",
    "            prediction_labels=pred_labels.astype(np.int8),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        print(f\"Saved preprocessed data for {patient_id} to {save_path}\")\n",
    "    \n",
    "    def load_preprocessed_data(self, patient_id):\n",
    "        \"\"\"Load preprocessed data for a patient\"\"\"\n",
    "        load_path = f\"./preprocessed_data/{patient_id}_preprocessed.npz\"\n",
    "        \n",
    "        if os.path.exists(load_path):\n",
    "            data = np.load(load_path, allow_pickle=True)\n",
    "            return {\n",
    "                'windows': data['windows'],\n",
    "                'detection_labels': data['detection_labels'],\n",
    "                'prediction_labels': data['prediction_labels'],\n",
    "                'metadata': data['metadata']\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def create_train_val_test_split(self, all_patient_data, strategy='patient_wise'):\n",
    "        \"\"\"Create dataset splits\"\"\"\n",
    "        if strategy == 'patient_wise':\n",
    "            patient_ids = list(all_patient_data.keys())\n",
    "            np.random.shuffle(patient_ids)\n",
    "            \n",
    "            n_train = int(len(patient_ids) * self.config.TRAIN_RATIO)\n",
    "            n_val = int(len(patient_ids) * self.config.VAL_RATIO)\n",
    "            \n",
    "            train_patients = patient_ids[:n_train]\n",
    "            val_patients = patient_ids[n_train:n_train + n_val]\n",
    "            test_patients = patient_ids[n_train + n_val:]\n",
    "            \n",
    "            train_data = {pid: all_patient_data[pid] for pid in train_patients}\n",
    "            val_data = {pid: all_patient_data[pid] for pid in val_patients}\n",
    "            test_data = {pid: all_patient_data[pid] for pid in test_patients}\n",
    "            \n",
    "            return train_data, val_data, test_data\n",
    "        \n",
    "        else:\n",
    "            # Implement other splitting strategies if needed\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization and Analysis Functions\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def plot_data_distribution(self, all_patient_data):\n",
    "        \"\"\"Plot distribution of data across patients\"\"\"\n",
    "        patient_stats = []\n",
    "        \n",
    "        for patient_id, data in all_patient_data.items():\n",
    "            n_windows = len(data['detection_labels'])\n",
    "            n_seizure = np.sum(data['detection_labels'])\n",
    "            n_preictal = np.sum(data['prediction_labels'])\n",
    "            \n",
    "            patient_stats.append({\n",
    "                'patient': patient_id,\n",
    "                'total_windows': n_windows,\n",
    "                'seizure_windows': n_seizure,\n",
    "                'preictal_windows': n_preictal,\n",
    "                'seizure_ratio': n_seizure / n_windows * 100,\n",
    "                'preictal_ratio': n_preictal / n_windows * 100\n",
    "            })\n",
    "        \n",
    "        stats_df = pd.DataFrame(patient_stats)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Total windows per patient\n",
    "        axes[0,0].bar(stats_df['patient'], stats_df['total_windows'])\n",
    "        axes[0,0].set_title('Total Windows per Patient')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Seizure windows\n",
    "        axes[0,1].bar(stats_df['patient'], stats_df['seizure_windows'], color='red')\n",
    "        axes[0,1].set_title('Seizure Windows per Patient')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Pre-ictal windows\n",
    "        axes[1,0].bar(stats_df['patient'], stats_df['preictal_windows'], color='orange')\n",
    "        axes[1,0].set_title('Pre-ictal Windows per Patient')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Class ratios\n",
    "        axes[1,1].bar(stats_df['patient'], stats_df['seizure_ratio'], alpha=0.7, label='Seizure %')\n",
    "        axes[1,1].bar(stats_df['patient'], stats_df['preictal_ratio'], alpha=0.7, label='Pre-ictal %')\n",
    "        axes[1,1].set_title('Class Distribution (%)')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return stats_df\n",
    "    \n",
    "    def plot_sample_windows(self, patient_data, patient_id, n_samples=5):\n",
    "        \"\"\"Plot sample EEG windows\"\"\"\n",
    "        windows = patient_data['windows']\n",
    "        det_labels = patient_data['detection_labels']\n",
    "        pred_labels = patient_data['prediction_labels']\n",
    "        \n",
    "        # Find samples of each class\n",
    "        seizure_idx = np.where(det_labels == 1)[0]\n",
    "        preictal_idx = np.where(pred_labels == 1)[0]\n",
    "        normal_idx = np.where((det_labels == 0) & (pred_labels == 0))[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(3, n_samples, figsize=(15, 9))\n",
    "        \n",
    "        # Plot seizure windows\n",
    "        if len(seizure_idx) > 0:\n",
    "            for i, idx in enumerate(seizure_idx[:n_samples]):\n",
    "                if i < n_samples:\n",
    "                    axes[0,i].plot(windows[idx, 0, :])  # Plot first channel\n",
    "                    axes[0,i].set_title(f'Seizure Window {i+1}')\n",
    "        \n",
    "        # Plot pre-ictal windows\n",
    "        if len(preictal_idx) > 0:\n",
    "            for i, idx in enumerate(preictal_idx[:n_samples]):\n",
    "                if i < n_samples:\n",
    "                    axes[1,i].plot(windows[idx, 0, :])\n",
    "                    axes[1,i].set_title(f'Pre-ictal Window {i+1}')\n",
    "        \n",
    "        # Plot normal windows\n",
    "        if len(normal_idx) > 0:\n",
    "            for i, idx in enumerate(normal_idx[:n_samples]):\n",
    "                if i < n_samples:\n",
    "                    axes[2,i].plot(windows[idx, 0, :])\n",
    "                    axes[2,i].set_title(f'Normal Window {i+1}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compute_dataset_statistics(self, all_patient_data):\n",
    "        \"\"\"Compute comprehensive dataset statistics\"\"\"\n",
    "        total_windows = 0\n",
    "        total_seizure = 0\n",
    "        total_preictal = 0\n",
    "        \n",
    "        for patient_id, data in all_patient_data.items():\n",
    "            total_windows += len(data['detection_labels'])\n",
    "            total_seizure += np.sum(data['detection_labels'])\n",
    "            total_preictal += np.sum(data['prediction_labels'])\n",
    "        \n",
    "        stats = {\n",
    "            'total_patients': len(all_patient_data),\n",
    "            'total_windows': total_windows,\n",
    "            'total_seizure_windows': total_seizure,\n",
    "            'total_preictal_windows': total_preictal,\n",
    "            'seizure_ratio': total_seizure / total_windows * 100,\n",
    "            'preictal_ratio': total_preictal / total_windows * 100,\n",
    "            'class_imbalance_ratio': (total_windows - total_seizure) / total_seizure if total_seizure > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(\"Dataset Statistics:\")\n",
    "        print(f\"Total Patients: {stats['total_patients']}\")\n",
    "        print(f\"Total Windows: {stats['total_windows']:,}\")\n",
    "        print(f\"Seizure Windows: {stats['total_seizure_windows']:,} ({stats['seizure_ratio']:.2f}%)\")\n",
    "        print(f\"Pre-ictal Windows: {stats['total_preictal_windows']:,} ({stats['preictal_ratio']:.2f}%)\")\n",
    "        print(f\"Class Imbalance Ratio: {stats['class_imbalance_ratio']:.2f}:1\")\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main Preprocessing Execution\n",
    "def main():\n",
    "    print(\"Starting CHB-MIT Scalp EEG Dataset Preprocessing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = CHBMITPreprocessor(config)\n",
    "    analyzer = DataAnalyzer(config)\n",
    "    \n",
    "    # Check if preprocessed data already exists\n",
    "    preprocessed_dir = \"./preprocessed_data\"\n",
    "    if os.path.exists(preprocessed_dir) and len(os.listdir(preprocessed_dir)) > 0:\n",
    "        print(\"Loading existing preprocessed data...\")\n",
    "        all_patient_data = {}\n",
    "        for file in os.listdir(preprocessed_dir):\n",
    "            if file.endswith('_preprocessed.npz'):\n",
    "                patient_id = file.replace('_preprocessed.npz', '')\n",
    "                patient_data = preprocessor.load_preprocessed_data(patient_id)\n",
    "                if patient_data is not None:\n",
    "                    all_patient_data[patient_id] = patient_data\n",
    "        \n",
    "        print(f\"Loaded data for {len(all_patient_data)} patients\")\n",
    "    else:\n",
    "        print(\"Preprocessing dataset from scratch...\")\n",
    "        all_patient_data = preprocessor.preprocess_complete_dataset()\n",
    "    \n",
    "    if not all_patient_data:\n",
    "        print(\"No data processed. Please check dataset path and structure.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze and visualize data\n",
    "    print(\"\\nAnalyzing dataset...\")\n",
    "    stats_df = analyzer.plot_data_distribution(all_patient_data)\n",
    "    overall_stats = analyzer.compute_dataset_statistics(all_patient_data)\n",
    "    \n",
    "    # Plot sample windows from first patient\n",
    "    first_patient = list(all_patient_data.keys())[0]\n",
    "    print(f\"\\nPlotting sample windows for {first_patient}...\")\n",
    "    analyzer.plot_sample_windows(all_patient_data[first_patient], first_patient)\n",
    "    \n",
    "    # Create dataset splits\n",
    "    print(\"\\nCreating dataset splits...\")\n",
    "    train_data, val_data, test_data = preprocessor.create_train_val_test_split(all_patient_data)\n",
    "    \n",
    "    print(f\"Training patients: {len(train_data)}\")\n",
    "    print(f\"Validation patients: {len(val_data)}\")\n",
    "    print(f\"Test patients: {len(test_data)}\")\n",
    "    \n",
    "    # Save dataset splits\n",
    "    split_info = {\n",
    "        'train_patients': list(train_data.keys()),\n",
    "        'val_patients': list(val_data.keys()),\n",
    "        'test_patients': list(test_data.keys())\n",
    "    }\n",
    "    \n",
    "    np.savez('./preprocessed_data/dataset_splits.npz', **split_info)\n",
    "    print(\"Dataset splits saved to ./preprocessed_data/dataset_splits.npz\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total patients processed: {len(all_patient_data)}\")\n",
    "    print(f\"Total windows: {overall_stats['total_windows']:,}\")\n",
    "    print(f\"Dataset ready for model training!\")\n",
    "    \n",
    "    return all_patient_data, train_data, val_data, test_data\n",
    "\n",
    "# Execute main preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    all_data, train_data, val_data, test_data = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sajan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
